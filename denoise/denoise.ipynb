{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c363cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from n2v.models import N2VConfig, N2V\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from csbdeep.utils import plot_history\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "MODEL_PATH = \"output/models\"\n",
    "OUTPUT_PATH = \"output/images\"\n",
    "def train_n2v_model_multi(train_images):\n",
    "    \"\"\"Train a Noise2Void model on multiple images\"\"\"\n",
    "    datagen = N2V_DataGenerator()\n",
    "    \n",
    "    # Process each training image\n",
    "    all_patches = []\n",
    "    \n",
    "    for img in train_images:\n",
    "        # Prepare the image for N2V - add channel dimension\n",
    "        img_for_patches = img[np.newaxis, ..., np.newaxis]\n",
    "        \n",
    "        # Generate patches for this image\n",
    "        patch_shape = img_for_patches.shape[1:]\n",
    "        patches = datagen.generate_patches_from_list([img_for_patches], shape=patch_shape)\n",
    "        all_patches.append(patches)\n",
    "    \n",
    "    # Combine all patches\n",
    "    X = np.concatenate(all_patches, axis=0)\n",
    "    print(f\"Generated a total of {X.shape[0]} training patches of shape {X.shape[1:]}\")\n",
    "    \n",
    "    # Split into training and validation (80/20)\n",
    "    np.random.shuffle(X)\n",
    "    n_train = int(0.8 * X.shape[0])\n",
    "    X_train, X_val = X[:n_train], X[n_train:]\n",
    "    print(f\"Training on {n_train} patches, validating on {X.shape[0] - n_train} patches\")\n",
    "    \n",
    "    # Configure N2V\n",
    "    config = N2VConfig(\n",
    "        X_train,\n",
    "        unet_kern_size=3,\n",
    "        train_steps_per_epoch=max(int(X_train.shape[0]/128), 10), \n",
    "        train_epochs=15,  # 15 epochs as requested\n",
    "        train_loss='mse',\n",
    "        batch_norm=True,\n",
    "        train_batch_size=128,\n",
    "        n2v_perc_pix=0.198,\n",
    "        n2v_patch_shape=(64, 64),\n",
    "        n2v_manipulator='uniform_withCP',\n",
    "        n2v_neighborhood_radius=5,\n",
    "        # N2V2 improvements\n",
    "        blurpool=True,\n",
    "        skip_skipone=True,\n",
    "        unet_residual=False\n",
    "    )\n",
    "    \n",
    "    # Create and train the model\n",
    "    model_name = 'n2v_cryoET_8slices'\n",
    "    model = N2V(config, model_name, basedir=MODEL_PATH)\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = model.train(X_train, X_val)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plot_history(history, ['loss', 'val_loss'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'training_history.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1879c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../parse_data/yolo_dataset/images/train\"\n",
    "img_extensions = ('.png', '.jpg', '.tif', '.tiff')\n",
    "img_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.lower().endswith(img_extensions)]\n",
    "TARGET_SHAPE = (64, 64)\n",
    "# --- 画像の読み込みと [0,1] への正規化 ---\n",
    "images = []\n",
    "for file in img_files:\n",
    "    img = io.imread(file)\n",
    "    # グレースケール画像の場合、チャネル次元を追加\n",
    "    if img.ndim == 2:\n",
    "        img = img[..., np.newaxis]\n",
    "    img = resize(\n",
    "        img,\n",
    "        (*TARGET_SHAPE, img.shape[-1]),\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=True\n",
    "    ).astype(np.float32)\n",
    "    # 画像を [0,1] の範囲に正規化\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    images.append(img)\n",
    "\n",
    "# 画像データを numpy 配列にまとめる: shape -> (n, H, W, C)\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d073e5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 64 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain_n2v_model_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_n2v_model_multi\u001b[39m\u001b[34m(train_images)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Generate patches for this image\u001b[39;00m\n\u001b[32m     28\u001b[39m     patch_shape = img_for_patches.shape[\u001b[32m1\u001b[39m:]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     patches = \u001b[43mdatagen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_patches_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_for_patches\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     all_patches.append(patches)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Combine all patches\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/tkdgk/BYU---Locating-Bacterial-Flagellar-Motors/.venv/lib/python3.12/site-packages/n2v/internals/N2V_DataGenerator.py:153\u001b[39m, in \u001b[36mN2V_DataGenerator.generate_patches_from_list\u001b[39m\u001b[34m(self, data, num_patches_per_img, shape, augment, shuffle)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(img.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m         p = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_patches_per_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m         patches.append(p)\n\u001b[32m    156\u001b[39m patches = np.concatenate(patches, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/tkdgk/BYU---Locating-Bacterial-Flagellar-Motors/.venv/lib/python3.12/site-packages/n2v/internals/N2V_DataGenerator.py:190\u001b[39m, in \u001b[36mN2V_DataGenerator.generate_patches\u001b[39m\u001b[34m(self, data, num_patches, shape, augment)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shape[-\u001b[32m2\u001b[39m] == shape[-\u001b[32m1\u001b[39m]:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         patches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__augment_patches__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m augment:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/tkdgk/BYU---Locating-Bacterial-Flagellar-Motors/.venv/lib/python3.12/site-packages/n2v/internals/N2V_DataGenerator.py:264\u001b[39m, in \u001b[36mN2V_DataGenerator.__augment_patches__\u001b[39m\u001b[34m(self, patches)\u001b[39m\n\u001b[32m    259\u001b[39m     augmented = np.concatenate((patches,\n\u001b[32m    260\u001b[39m                                 np.rot90(patches, k=\u001b[32m1\u001b[39m, axes=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)),\n\u001b[32m    261\u001b[39m                                 np.rot90(patches, k=\u001b[32m2\u001b[39m, axes=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)),\n\u001b[32m    262\u001b[39m                                 np.rot90(patches, k=\u001b[32m3\u001b[39m, axes=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))))\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(patches.shape[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]) == \u001b[32m3\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     augmented = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrot90\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrot90\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrot90\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m augmented = np.concatenate((augmented, np.flip(augmented, axis=-\u001b[32m2\u001b[39m)))\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m augmented\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 64 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "model = train_n2v_model_multi(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39aa48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images shape: (3262, 256, 256, 1)\n",
      "Training set shape: (2609, 256, 256, 1)\n",
      "Validation set shape: (653, 256, 256, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation set shape:\u001b[39m\u001b[33m\"\u001b[39m, x_val.shape)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# --- Noise2Void モデルの生成 ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m model = \u001b[43mN2V\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#name='n2v_cryoET_8slices', \u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#basedir=MODEL_PATH\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# --- 学習パラメータの設定 ---\u001b[39;00m\n\u001b[32m     51\u001b[39m epochs = \u001b[32m200\u001b[39m            \u001b[38;5;66;03m# エポック数\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/tkdgk/BYU---Locating-Bacterial-Flagellar-Motors/.venv/lib/python3.12/site-packages/n2v/models/n2v_standard.py:88\u001b[39m, in \u001b[36mN2V.__init__\u001b[39m\u001b[34m(self, config, name, basedir)\u001b[39m\n\u001b[32m     85\u001b[39m basedir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(basedir, (string_types, Path)) \u001b[38;5;129;01mor\u001b[39;00m _raise(\n\u001b[32m     86\u001b[39m     \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo valid basedir: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mstr\u001b[39m(basedir)))\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28mself\u001b[39m.name = name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatetime\u001b[49m.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m.basedir = Path(basedir) \u001b[38;5;28;01mif\u001b[39;00m basedir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# config was provided -> update before it is saved to disk\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from n2v.models import N2V\n",
    "\n",
    "# --- パスやパラメータの設定 ---\n",
    "MODEL_PATH = 'output/denoise_model/'\n",
    "input_dir = \"../parse_data/yolo_dataset/images/train\"\n",
    "\n",
    "# 対象とする画像拡張子の定義\n",
    "img_extensions = ('.png', '.jpg', '.tif', '.tiff')\n",
    "img_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.lower().endswith(img_extensions)]\n",
    "TARGET_SHAPE = (256, 256)\n",
    "# --- 画像の読み込みと [0,1] への正規化 ---\n",
    "images = []\n",
    "for file in img_files:\n",
    "    img = io.imread(file)\n",
    "    # グレースケール画像の場合、チャネル次元を追加\n",
    "    if img.ndim == 2:\n",
    "        img = img[..., np.newaxis]\n",
    "    img = resize(\n",
    "        img,\n",
    "        (*TARGET_SHAPE, img.shape[-1]),\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=True\n",
    "    ).astype(np.float32)\n",
    "    # 画像を [0,1] の範囲に正規化\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    images.append(img)\n",
    "\n",
    "# 画像データを numpy 配列にまとめる: shape -> (n, H, W, C)\n",
    "images = np.array(images)\n",
    "print(\"Loaded images shape:\", images.shape)\n",
    "\n",
    "# --- 訓練用と検証用データへの分割 ---\n",
    "split_ratio = 0.8  # 80% を訓練、20% を検証に使用\n",
    "split_index = int(len(images) * split_ratio)\n",
    "x_train = images[:split_index]\n",
    "x_val = images[split_index:]\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Validation set shape:\", x_val.shape)\n",
    "\n",
    "# --- Noise2Void モデルの生成 ---\n",
    "model = N2V(config=None, \n",
    "            #name='n2v_cryoET_8slices', \n",
    "            #basedir=MODEL_PATH\n",
    "            )\n",
    "\n",
    "# --- 学習パラメータの設定 ---\n",
    "epochs = 200            # エポック数\n",
    "steps_per_epoch = 200   # 1 エポックあたりのステップ数\n",
    "batch_size = 4          # バッチサイズ\n",
    "\n",
    "# --- モデルの学習 ---\n",
    "model.train(x_train, x_val, steps_per_epoch=steps_per_epoch, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing function for denoising multiple slices\n",
    "def batch_process_n2v(input_dir, output_dir, slice_range=None):\n",
    "    \"\"\"\n",
    "    Process a batch of slices with the pre-trained Noise2Void model\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir: Directory containing input JPEG slices\n",
    "        output_dir: Directory to save denoised slices\n",
    "        slice_range: Optional tuple (start, end) to process only a subset of slices\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all JPEG files in the input directory\n",
    "    all_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Apply slice range if specified\n",
    "    if slice_range is not None:\n",
    "        start, end = slice_range\n",
    "        all_files = all_files[start:end]\n",
    "    \n",
    "    print(f\"Processing {len(all_files)} slices...\")\n",
    "    \n",
    "    # Process each slice\n",
    "    for filename in all_files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, f\"denoised_{filename}\")\n",
    "        \n",
    "        # Load image\n",
    "        img = np.array(Image.open(input_path).convert('L'))\n",
    "        \n",
    "        # Add dimensions for N2V (SYXC format)\n",
    "        img_for_pred = img[np.newaxis, ..., np.newaxis]\n",
    "        \n",
    "        # Apply Noise2Void denoising\n",
    "        denoised = model.predict(img_for_pred, axes='SYXC')\n",
    "        \n",
    "        # Remove batch and channel dimensions\n",
    "        denoised_img = denoised[0, ..., 0]\n",
    "        \n",
    "        # Convert to uint8 for saving\n",
    "        if denoised_img.dtype != np.uint8:\n",
    "            denoised_img = np.clip(denoised_img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Save denoised image\n",
    "        Image.fromarray(denoised_img).save(output_path)\n",
    "    \n",
    "    print(f\"Batch processing complete. Denoised images saved to {output_dir}\")\n",
    "\n",
    "# tomo_id = '00e047'\n",
    "# input_dir = f'/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train/tomo_{tomo_id}'\n",
    "# output_dir = f'/kaggle/working/denoised_tomo_{tomo_id}'\n",
    "# batch_process_n2v(input_dir, output_dir, slice_range=(100, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../parse_data/yolo_dataset/images/train\"\n",
    "output_dir = \"yolo_dataset/images/train\"\n",
    "batch_process_n2v(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../parse_data/yolo_dataset/images/val\"\n",
    "output_dir = \"yolo_dataset/images/val\"\n",
    "batch_process_n2v(input_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
